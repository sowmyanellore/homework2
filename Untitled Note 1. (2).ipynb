{
  "metadata": {
    "name": "Untitled Note 1",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport math\ndef repart_df(day):\n    df \u003d spark.read.csv(\"s3://rdsdump.scanlife.com/location_partitioned/provider\u003dlocation/source\u003dirys/month\u003d2022-01/day\u003d{:02d}/\".format(day))\n    df \u003d df.distinct()\n    cnt \u003d df.count()\n    no_of_partitions\u003dint(math.ceil(float(cnt)/8000000))\n    df.repartition(no_of_partitions).write.option(\"header\",True).csv(\"s3://rdsdump.scanlife.com/location_partitioned/provider\u003dlocation/source\u003dirys/month\u003d2022-01_temp/day\u003d{:02d}/\".format(day))"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfor i in range(2,32):\n    repart_df(i)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}